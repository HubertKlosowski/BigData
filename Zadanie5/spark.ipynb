{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Podstawy Sparka\n",
    "### Hubert Kłosowski 242424\n",
    "### Kamil Małecki 242464"
   ],
   "id": "fd66562510928cc6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### <center>Cześć 1</center>",
   "id": "58da82160e846737"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.appName('Czesc 1 BigData').getOrCreate()\n",
    "\n",
    "print(spark.version)"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "data = [i + 1 for i in range(5)]\n",
    "\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "\n",
    "print('RDD:', rdd.collect())"
   ],
   "id": "8cc49b43861764cf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df = spark.createDataFrame([('Andorra', 98), ('United Arab Emirates', 1683)], ['country_name', 'total_confirmed'])\n",
    "\n",
    "df.show()"
   ],
   "id": "1ca5da1c7d9b8e3f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "spark.stop()",
   "id": "fc5b4fb2663b508f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### <center>Część 2</center>",
   "id": "6179c6303bb59fb0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2.1. Wczytaj dane ze swojego pliku CSV dotyczącego danych COVID-19 (pamiętaj o nagłówku dla kolumn).",
   "id": "dac18a39788e89e7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "covid_df = pd.read_csv('data/covid.csv')\n",
    "\n",
    "def save_spark_df(df, name):\n",
    "    if 'result' not in os.listdir(os.getcwd()):\n",
    "        os.mkdir('result')\n",
    "    df.write.csv(f'{name}.csv', header=True)\n",
    "    df.write.json(f'{name}.json')\n",
    "    df.write.parquet(f'{name}.parquet')\n",
    "    df.write.format('avro').save(f'{name}.avro')"
   ],
   "id": "c1280f8964013780",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2.3. Wyświetl co najmniej 5 kolumn i wyjaśnij ich znaczenie dla analizy przypadków COVID-19. ",
   "id": "81c6b86db12dc0eb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "covid_df = covid_df[['date', 'country_name', 'Continent', 'age_confirmed_0', 'age_confirmed_1', 'age_deceased_0', 'age_deceased_1', 'health_expenditure_usd', 'life_expectancy', 'school_closing']]",
   "id": "3bd8127e5bf8d732",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2.2. Utwórz DataFrame. Pamiętaj o utworzeniu własnego schematu z wybranymi kolumnami. ",
   "id": "f9c91feaa82ad767"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.appName('Czesc 2 BigData').getOrCreate()\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('date', StringType(), True),\n",
    "    StructField('country_name', StringType(), True),\n",
    "    StructField('continent', StringType(), True),\n",
    "    StructField('preschoolers_confirmed', IntegerType(), True),\n",
    "    StructField('pupils_confirmed', IntegerType(), True),\n",
    "    StructField('preschoolers_dead', IntegerType(), True),\n",
    "    StructField('pupils_dead', IntegerType(), True),\n",
    "    StructField('health_expenditure_usd', FloatType(), True),\n",
    "    StructField('life_expectancy', FloatType(), True),\n",
    "    StructField('school_closing', IntegerType(), True),\n",
    "])\n",
    "\n",
    "spark_covid_df = spark.createDataFrame(covid_df, schema)"
   ],
   "id": "ac3d9aaaf848a885",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2.3. Wyświetl co najmniej 5 kolumn i wyjaśnij ich znaczenie dla analizy przypadków COVID-19.",
   "id": "b23194ffcc9cd918"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "save_spark_df(spark_covid_df, name='2_3')\n",
    "spark_covid_df.show(5)"
   ],
   "id": "33f4f7339161176",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2.4. Dodaj kolumnę, która będzie zbierała dane, które uznasz za stosowne i które są istotne z punktu widzenia analizy COVID-19 (np. wyliczenie gęstości zaludnienia na podstawie liczby ludności i powierzchni kraju).",
   "id": "f5a4e27cacc23e6a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pyspark.sql.functions import concat, col\n",
    "\n",
    "\n",
    "spark_covid_df = spark_covid_df.withColumn(\n",
    "    'who_died_more', \n",
    "    (col('preschoolers_dead') - col('pupils_dead')) * 100\n",
    ").withColumn(\n",
    "    'who_ill_more', \n",
    "    (col('preschoolers_confirmed') - col('pupils_confirmed')) * 100\n",
    ")\n",
    "\n",
    "save_spark_df(spark_covid_df, name='2_4')\n",
    "spark_covid_df.show(5)"
   ],
   "id": "6734f316bfe280",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2.5. Połącz wybrane kolumny ze sobą w sposób niosący nowe informacje za pomocą funkcji concat .",
   "id": "8b5e19c34f5fcfb7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "spark_covid_df = spark_covid_df.withColumn(\n",
    "    'combined_country_continent',\n",
    "    concat(col('country_name'), col('continent')),\n",
    ")\n",
    "\n",
    "save_spark_df(spark_covid_df, name='2_5')\n",
    "spark_covid_df.show(5)"
   ],
   "id": "60dcfbe0c3b329bc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2.6. Wyfiltruj co najmniej 10 wybranych przez siebie, istotnych informacji (np. kraj z liczbą łóżek większą od 10000 czy dzień, w którym było najmniej zachorowań w danym kraju).",
   "id": "4381b1dddc2c609a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "filtered_df = spark_covid_df.filter((col('pupils_confirmed') > 100) & (col('preschoolers_dead') > 100))\n",
    "\n",
    "save_spark_df(filtered_df, name='2_6')\n",
    "filtered_df.show(5)"
   ],
   "id": "1545d3c5e13ff14c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2.7. Wykonaj 5 agregacji na swoim DF (np. min, max, agg, count, mean) uprzednio pogrupowawszy kolumny (groupBy).",
   "id": "5f82a40964c7adba"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "aggregated_df = spark_covid_df.groupBy(\"country_name\").agg(\n",
    "    {\"preschoolers_confirmed\": \"max\",\n",
    "     \"pupils_confirmed\": \"sum\",\n",
    "     \"preschoolers_dead\": \"avg\",\n",
    "     \"pupils_dead\": \"min\",\n",
    "     \"health_expenditure_usd\": \"mean\"}\n",
    ")\n",
    "\n",
    "save_spark_df(aggregated_df, name='2_7')\n",
    "aggregated_df.show(5)"
   ],
   "id": "adb48f5bd297c079",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
